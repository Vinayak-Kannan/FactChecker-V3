{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the folder containing ClusterAndPredict\n",
    "module_path = os.path.abspath(os.path.join('/home/ec2-user/SageMaker', 'factcheck-model-v2'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "from ClusterAndPredict.ClusterAndPredict import ClusterAndPredict\n",
    "from Testing.DataLoader import DataLoader\n",
    "from Testing.ParameterCreator import ParameterCreator\n",
    "from Clustering.Helpers.Visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./../../Clustering/Clustering/Chroma\")\n",
    "# # Count number of collections\n",
    "# print(chroma_client.count_collections())\n",
    "# \n",
    "# # Get all collection names\n",
    "# collection_names = chroma_client.list_collections()\n",
    "# \n",
    "# # Loop through each collection and drop it\n",
    "# for collection_name in collection_names:\n",
    "#     if collection_name.name != 'climate_claims_embeddings_unchanged':\n",
    "#         chroma_client.delete_collection(collection_name.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParameterCreator().get_parameters()\n",
    "results = []\n",
    "cluster_dfs = []\n",
    "print(\"Number of experiments to run: \", len(params))\n",
    "for param in params:\n",
    "    percentage = 0.75\n",
    "    data_loader = DataLoader(percentage, True, param['random_seed'])\n",
    "    use_only_card = param['use_only_CARD']\n",
    "    size_of_dataset = param['size_of_dataset']\n",
    "    del param['size_of_dataset']\n",
    "    del param['use_only_CARD']\n",
    "\n",
    "    train_df, test_df = data_loader.create_train_test_df(False, False, False, True, size_of_dataset)\n",
    "    if use_only_card:\n",
    "        print(\"using card\")\n",
    "        train_df, test_df = data_loader.create_train_test_df(True, True, True, True, size_of_dataset)\n",
    "\n",
    "    clf = ClusterAndPredict(**param, train_df=train_df)\n",
    "    clf.fit(test_df['Text'].tolist(), test_df['Numerical Rating'].tolist())\n",
    "    # Print best parameters\n",
    "    best_estimator = clf\n",
    "    score = best_estimator.score([], [])\n",
    "    print(best_estimator.get_all_performance_metrics())\n",
    "    object_output = best_estimator.get_all_performance_metrics()\n",
    "    cluster_df = object_output['cluster_df']\n",
    "    cluster_dfs.append(cluster_df)\n",
    "    output = {\n",
    "        'percentage': percentage,\n",
    "        'score': score,\n",
    "        'accuracy': best_estimator.get_accuracy(),\n",
    "        'was_supervised_umap_used': best_estimator.get_was_supervised(),\n",
    "        'metrics': best_estimator.get_all_performance_metrics(),\n",
    "        'size_of_dataset': size_of_dataset,\n",
    "        'use_only_CARD': use_only_card\n",
    "    }\n",
    "    # Prepend the value 'param' to the keys in params\n",
    "    for key, value in param.items():\n",
    "        local_key = 'params.' + key\n",
    "        output[local_key] = value\n",
    "\n",
    "    results.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = Visualizer()\n",
    "df_with_two_dimens = viz.fit_transform(cluster_dfs[0], 'embeddings')\n",
    "df_with_two_dimens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件输入输出模板\n",
    "\n",
    "import boto3\n",
    "\n",
    "# 创建 S3 客户端\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'sagemaker-us-east-1-390403859474'\n",
    "\n",
    "s3_file_key = 'processed_file.csv'  # S3 中的文件路径\n",
    "\n",
    "local_file_path = 'downloaded_file.csv'  # 本地保存的文件名\n",
    "\n",
    "# 下载文件\n",
    "s3.download_file(bucket_name, s3_file_key, local_file_path)\n",
    "\n",
    "print(f\"File {s3_file_key} has been downloaded to {local_file_path}.\")\n",
    "\n",
    "processed_file_path = f'results_${str(now)}.csv'\n",
    "results_df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 定义上传路径\n",
    "upload_file_key = 'processed_file.csv'  # 上传到 S3 的路径\n",
    "\n",
    "# 上传文件\n",
    "s3.upload_file(processed_file_path, bucket_name, upload_file_key)\n",
    "\n",
    "print(f\"File {processed_file_path} has been uploaded to s3://{bucket_name}/{upload_file_key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator.clusters_df.head(100).to_csv(\"test1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Get access, secret key, and bucket name from environment variables\n",
    "AWS_ACCESS_KEY = os.environ['AWS_ACCESS_KEY']\n",
    "AWS_SECRET_KEY = os.environ['AWS_SECRET_KEY']\n",
    "BUCKET_NAME = os.environ['BUCKET_NAME']\n",
    "\n",
    "# Data File\n",
    "CSV_FILE_PATH = 'test1.csv'\n",
    "\n",
    "# Connect to S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Create local temp file\n",
    "TEMP_DIR = 'temp_files'\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Read CSV file and get column name\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "batch_size = 10\n",
    "total_batches = (len(data) + batch_size - 1) // batch_size\n",
    "upload_count = 0\n",
    "\n",
    "for i in range(total_batches):\n",
    "    \n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(data))\n",
    "    df = data.iloc[start_idx:end_idx]\n",
    "    column_names = df.columns\n",
    "    \n",
    "    # Create files contains single claim\n",
    "    for index, row in df.iterrows():\n",
    "        text = str(df.loc[index, 'text'])\n",
    "        filtered_text = ''.join(re.findall(r'[A-Za-z0-9]', text))\n",
    "        sub_name = filtered_text[:30]\n",
    "        file_name = f\"claim_{sub_name}.csv\"\n",
    "        file_path = os.path.join(TEMP_DIR, file_name)\n",
    "        \n",
    "        single_row_df = pd.DataFrame([row], columns=column_names)\n",
    "        single_row_df.to_csv(file_path, index=False)\n",
    "        \n",
    "        s3_key = f\"groundtruth/{file_name}\"\n",
    "        \n",
    "        # Check if the file exists in S3\n",
    "        try:\n",
    "            s3_client.head_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "            # File exists, download it\n",
    "            s3_object = s3_client.get_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "            s3_data = pd.read_csv(io.BytesIO(s3_object['Body'].read()))\n",
    "            \n",
    "            # Compare the data\n",
    "            if single_row_df.equals(s3_data):\n",
    "                print(f\"{file_name} is up-to-date. Skipping upload.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"{file_name} is outdated. Uploading new version.\")\n",
    "        except s3_client.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                # File does not exist\n",
    "                print(f\"{file_name} does not exist in S3. Uploading new file.\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_client.upload_file(\n",
    "            Filename=file_path,\n",
    "            Bucket=BUCKET_NAME,\n",
    "            Key=s3_key  # S3 path\n",
    "        )\n",
    "        print(f\"Uploaded {file_name} to S3.\")\n",
    "        upload_count += 1\n",
    "\n",
    "        # Rest if upload count equals batch_size\n",
    "        if upload_count == batch_size:\n",
    "            print(\"Rest for 180s...\")\n",
    "            time.sleep(180)\n",
    "            upload_count = 0\n",
    "\n",
    "# Delete temp files\n",
    "for file in os.listdir(TEMP_DIR):\n",
    "    os.remove(os.path.join(TEMP_DIR, file))\n",
    "os.rmdir(TEMP_DIR)\n",
    "\n",
    "print(\"All files uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# Get S3 and object key from environment variables\n",
    "bucket = os.environ['BUCKET_S3']\n",
    "bucket_key = os.environ['BUCKET_S3_KEY']\n",
    "\n",
    "\n",
    "# 使用 boto3 客户端\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# 下载文件到本地\n",
    "s3.download_file(bucket, key, 'data.json.gz')\n",
    "\n",
    "# 解压缩并逐行读取数据\n",
    "data = []\n",
    "with gzip.open('data.json.gz', 'rt') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# 如果数据为 DataFrame 格式的，可以转为 pandas DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "df.to_csv(\"GoogleFactCheckData.csv\")\n",
    "\n",
    "# 查看数据\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
