{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: python-dotenv in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.58.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: pinecone in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (5.4.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (2024.8.30)\n",
      "Requirement already satisfied: pinecone-plugin-inference<4.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (3.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (2.9.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n",
      "Requirement already satisfied: umap-learn in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (4.67.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Requirement already satisfied: hdbscan in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (0.8.40)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn>=0.20->hdbscan) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install pinecone\n",
    "!pip install umap-learn\n",
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 10:55:41.557012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-21 10:55:41.583732: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-21 10:55:41.583776: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-21 10:55:41.600346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-21 10:55:43.363383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the folder containing ClusterAndPredict\n",
    "module_path = os.path.abspath(os.path.join('/home/ec2-user/SageMaker', 'FactChecker-V3'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "from ClusterAndPredict.ClusterAndPredict import ClusterAndPredict\n",
    "from Testing.DataLoader import DataLoader\n",
    "from Testing.ParameterCreator import ParameterCreator\n",
    "from Clustering.Helpers.Visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./../../Clustering/Clustering/Chroma\")\n",
    "# # Count number of collections\n",
    "# print(chroma_client.count_collections())\n",
    "# \n",
    "# # Get all collection names\n",
    "# collection_names = chroma_client.list_collections()\n",
    "# \n",
    "# # Loop through each collection and drop it\n",
    "# for collection_name in collection_names:\n",
    "#     if collection_name.name != 'climate_claims_embeddings_unchanged':\n",
    "#         chroma_client.delete_collection(collection_name.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data sources are being used\n",
      "1\n",
      "Number of experiments to run:  1\n",
      "3    3256\n",
      "1      28\n",
      "Name: Numerical Rating, dtype: int64\n",
      "3    771\n",
      "1     51\n",
      "Name: Numerical Rating, dtype: int64\n",
      "using card\n",
      "All data sources are being used\n",
      "1\n",
      "3    4414\n",
      "1    2516\n",
      "Name: Numerical Rating, dtype: int64\n",
      "3    894\n",
      "1    839\n",
      "Name: Numerical Rating, dtype: int64\n",
      "Fitting\n",
      "Getting embeddings...\n",
      "Getting embeddings for batch  0  out of  1739\n",
      "Getting embeddings for batch  50  out of  1739\n",
      "Getting embeddings for batch  100  out of  1739\n",
      "Getting embeddings for batch  150  out of  1739\n",
      "Getting embeddings for batch  200  out of  1739\n",
      "Getting embeddings for batch  250  out of  1739\n",
      "Getting embeddings for batch  300  out of  1739\n",
      "Getting embeddings for batch  350  out of  1739\n",
      "Getting embeddings for batch  400  out of  1739\n",
      "Getting embeddings for batch  450  out of  1739\n",
      "Getting embeddings for batch  500  out of  1739\n",
      "Getting embeddings for batch  550  out of  1739\n",
      "Getting embeddings for batch  600  out of  1739\n",
      "Getting embeddings for batch  650  out of  1739\n",
      "Getting embeddings for batch  700  out of  1739\n",
      "Getting embeddings for batch  750  out of  1739\n",
      "Getting embeddings for batch  800  out of  1739\n",
      "Getting embeddings for batch  850  out of  1739\n",
      "Getting embeddings for batch  900  out of  1739\n",
      "Getting embeddings for batch  950  out of  1739\n",
      "Getting embeddings for batch  1000  out of  1739\n",
      "Getting embeddings for batch  1050  out of  1739\n",
      "Getting embeddings for batch  1100  out of  1739\n",
      "Getting embeddings for batch  1150  out of  1739\n",
      "Getting embeddings for batch  1200  out of  1739\n",
      "Getting embeddings for batch  1250  out of  1739\n",
      "Getting embeddings for batch  1300  out of  1739\n",
      "Getting embeddings for batch  1350  out of  1739\n",
      "Getting embeddings for batch  1400  out of  1739\n",
      "Getting embeddings for batch  1450  out of  1739\n",
      "Getting embeddings for batch  1500  out of  1739\n",
      "Getting embeddings for batch  1550  out of  1739\n",
      "Getting embeddings for batch  1600  out of  1739\n",
      "Getting embeddings for batch  1650  out of  1739\n",
      "Getting embeddings for batch  1700  out of  1739\n",
      "Getting embeddings for batch  0  out of  6930\n",
      "Getting embeddings for batch  50  out of  6930\n",
      "Getting embeddings for batch  100  out of  6930\n",
      "Getting embeddings for batch  150  out of  6930\n",
      "Getting embeddings for batch  200  out of  6930\n",
      "Getting embeddings for batch  250  out of  6930\n",
      "Getting embeddings for batch  300  out of  6930\n",
      "Getting embeddings for batch  350  out of  6930\n",
      "Getting embeddings for batch  400  out of  6930\n",
      "Getting embeddings for batch  450  out of  6930\n",
      "Getting embeddings for batch  500  out of  6930\n",
      "Getting embeddings for batch  550  out of  6930\n",
      "Getting embeddings for batch  600  out of  6930\n",
      "Getting embeddings for batch  650  out of  6930\n",
      "Getting embeddings for batch  700  out of  6930\n",
      "Getting embeddings for batch  750  out of  6930\n",
      "Getting embeddings for batch  800  out of  6930\n",
      "Getting embeddings for batch  850  out of  6930\n",
      "Getting embeddings for batch  900  out of  6930\n",
      "Getting embeddings for batch  950  out of  6930\n",
      "Getting embeddings for batch  1000  out of  6930\n",
      "Getting embeddings for batch  1050  out of  6930\n",
      "Getting embeddings for batch  1100  out of  6930\n",
      "Getting embeddings for batch  1150  out of  6930\n",
      "Getting embeddings for batch  1200  out of  6930\n",
      "Getting embeddings for batch  1250  out of  6930\n",
      "Getting embeddings for batch  1300  out of  6930\n",
      "Getting embeddings for batch  1350  out of  6930\n",
      "Getting embeddings for batch  1400  out of  6930\n",
      "Getting embeddings for batch  1450  out of  6930\n",
      "Getting embeddings for batch  1500  out of  6930\n",
      "Getting embeddings for batch  1550  out of  6930\n",
      "Getting embeddings for batch  1600  out of  6930\n",
      "Getting embeddings for batch  1650  out of  6930\n",
      "Getting embeddings for batch  1700  out of  6930\n",
      "Getting embeddings for batch  1750  out of  6930\n",
      "Getting embeddings for batch  1800  out of  6930\n",
      "Getting embeddings for batch  1850  out of  6930\n",
      "Getting embeddings for batch  1900  out of  6930\n",
      "Getting embeddings for batch  1950  out of  6930\n",
      "Getting embeddings for batch  2000  out of  6930\n",
      "Getting embeddings for batch  2050  out of  6930\n",
      "Getting embeddings for batch  2100  out of  6930\n",
      "Getting embeddings for batch  2150  out of  6930\n",
      "Getting embeddings for batch  2200  out of  6930\n",
      "Getting embeddings for batch  2250  out of  6930\n",
      "Getting embeddings for batch  2300  out of  6930\n",
      "Getting embeddings for batch  2350  out of  6930\n",
      "Getting embeddings for batch  2400  out of  6930\n",
      "Getting embeddings for batch  2450  out of  6930\n",
      "Getting embeddings for batch  2500  out of  6930\n",
      "Getting embeddings for batch  2550  out of  6930\n",
      "Getting embeddings for batch  2600  out of  6930\n",
      "Getting embeddings for batch  2650  out of  6930\n",
      "Getting embeddings for batch  2700  out of  6930\n",
      "Getting embeddings for batch  2750  out of  6930\n",
      "Getting embeddings for batch  2800  out of  6930\n",
      "Getting embeddings for batch  2850  out of  6930\n",
      "Getting embeddings for batch  2900  out of  6930\n",
      "Getting embeddings for batch  2950  out of  6930\n",
      "Getting embeddings for batch  3000  out of  6930\n",
      "Getting embeddings for batch  3050  out of  6930\n",
      "Getting embeddings for batch  3100  out of  6930\n",
      "Getting embeddings for batch  3150  out of  6930\n",
      "Getting embeddings for batch  3200  out of  6930\n",
      "Getting embeddings for batch  3250  out of  6930\n",
      "Getting embeddings for batch  3300  out of  6930\n",
      "Getting embeddings for batch  3350  out of  6930\n",
      "Getting embeddings for batch  3400  out of  6930\n",
      "Getting embeddings for batch  3450  out of  6930\n",
      "Getting embeddings for batch  3500  out of  6930\n",
      "Getting embeddings for batch  3550  out of  6930\n",
      "Getting embeddings for batch  3600  out of  6930\n",
      "Getting embeddings for batch  3650  out of  6930\n",
      "Getting embeddings for batch  3700  out of  6930\n",
      "Getting embeddings for batch  3750  out of  6930\n",
      "Getting embeddings for batch  3800  out of  6930\n",
      "Getting embeddings for batch  3850  out of  6930\n",
      "Getting embeddings for batch  3900  out of  6930\n",
      "Getting embeddings for batch  3950  out of  6930\n",
      "Getting embeddings for batch  4000  out of  6930\n",
      "Getting embeddings for batch  4050  out of  6930\n",
      "Getting embeddings for batch  4100  out of  6930\n",
      "Getting embeddings for batch  4150  out of  6930\n",
      "Getting embeddings for batch  4200  out of  6930\n",
      "Getting embeddings for batch  4250  out of  6930\n",
      "Getting embeddings for batch  4300  out of  6930\n",
      "Getting embeddings for batch  4350  out of  6930\n",
      "Getting embeddings for batch  4400  out of  6930\n",
      "Getting embeddings for batch  4450  out of  6930\n",
      "Getting embeddings for batch  4500  out of  6930\n",
      "Getting embeddings for batch  4550  out of  6930\n",
      "Getting embeddings for batch  4600  out of  6930\n",
      "Getting embeddings for batch  4650  out of  6930\n",
      "Getting embeddings for batch  4700  out of  6930\n",
      "Getting embeddings for batch  4750  out of  6930\n",
      "Getting embeddings for batch  4800  out of  6930\n",
      "Getting embeddings for batch  4850  out of  6930\n",
      "Getting embeddings for batch  4900  out of  6930\n",
      "Getting embeddings for batch  4950  out of  6930\n",
      "Getting embeddings for batch  5000  out of  6930\n",
      "Getting embeddings for batch  5050  out of  6930\n",
      "Getting embeddings for batch  5100  out of  6930\n",
      "Getting embeddings for batch  5150  out of  6930\n",
      "Getting embeddings for batch  5200  out of  6930\n",
      "Getting embeddings for batch  5250  out of  6930\n",
      "Getting embeddings for batch  5300  out of  6930\n",
      "Getting embeddings for batch  5350  out of  6930\n",
      "Getting embeddings for batch  5400  out of  6930\n",
      "Getting embeddings for batch  5450  out of  6930\n",
      "Getting embeddings for batch  5500  out of  6930\n",
      "Getting embeddings for batch  5550  out of  6930\n",
      "Getting embeddings for batch  5600  out of  6930\n",
      "Getting embeddings for batch  5650  out of  6930\n",
      "Getting embeddings for batch  5700  out of  6930\n",
      "Getting embeddings for batch  5750  out of  6930\n",
      "Getting embeddings for batch  5800  out of  6930\n",
      "Getting embeddings for batch  5850  out of  6930\n",
      "Getting embeddings for batch  5900  out of  6930\n",
      "Getting embeddings for batch  5950  out of  6930\n",
      "Getting embeddings for batch  6000  out of  6930\n",
      "Getting embeddings for batch  6050  out of  6930\n",
      "Getting embeddings for batch  6100  out of  6930\n",
      "Getting embeddings for batch  6150  out of  6930\n",
      "Getting embeddings for batch  6200  out of  6930\n",
      "Getting embeddings for batch  6250  out of  6930\n",
      "Getting embeddings for batch  6300  out of  6930\n",
      "Getting embeddings for batch  6350  out of  6930\n",
      "Getting embeddings for batch  6400  out of  6930\n",
      "Getting embeddings for batch  6450  out of  6930\n",
      "Getting embeddings for batch  6500  out of  6930\n",
      "Getting embeddings for batch  6550  out of  6930\n",
      "Getting embeddings for batch  6600  out of  6930\n",
      "Getting embeddings for batch  6650  out of  6930\n",
      "Getting embeddings for batch  6700  out of  6930\n",
      "Getting embeddings for batch  6750  out of  6930\n",
      "Getting embeddings for batch  6800  out of  6930\n",
      "Getting embeddings for batch  6850  out of  6930\n",
      "Getting embeddings for batch  6900  out of  6930\n",
      "temp_df length: 7275\n",
      "claims_embeddings shape:  (1739, 3072)\n",
      "claims_embeddings predict shape:  (5536, 3072)\n",
      "Running parametric supervised umap...\n",
      "seed: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 11:05:36.474692: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 178790400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights file not found at encoder.weights.h5. Running fit() instead.\n"
     ]
    }
   ],
   "source": [
    "params = ParameterCreator().get_parameters()\n",
    "results = []\n",
    "cluster_dfs = []\n",
    "print(\"Number of experiments to run: \", len(params))\n",
    "for param in params:\n",
    "    percentage = 0.75\n",
    "    data_loader = DataLoader(percentage, True, param['random_seed'])\n",
    "    use_only_card = param['use_only_CARD']\n",
    "    size_of_dataset = param['size_of_dataset']\n",
    "    del param['size_of_dataset']\n",
    "    del param['use_only_CARD']\n",
    "\n",
    "    train_df, test_df = data_loader.create_train_test_df(False, False, False, True, size_of_dataset)\n",
    "    if use_only_card:\n",
    "        print(\"using card\")\n",
    "        train_df, test_df = data_loader.create_train_test_df(True, True, True, True, size_of_dataset)\n",
    "\n",
    "    clf = ClusterAndPredict(**param, train_df=train_df)\n",
    "    clf.fit(test_df['Text'].tolist(), test_df['Numerical Rating'].tolist())\n",
    "    # Print best parameters\n",
    "    best_estimator = clf\n",
    "    score = best_estimator.score([], [])\n",
    "    print(best_estimator.get_all_performance_metrics())\n",
    "    object_output = best_estimator.get_all_performance_metrics()\n",
    "    cluster_df = object_output['cluster_df']\n",
    "    cluster_dfs.append(cluster_df)\n",
    "    output = {\n",
    "        'percentage': percentage,\n",
    "        'score': score,\n",
    "        'accuracy': best_estimator.get_accuracy(),\n",
    "        'was_supervised_umap_used': best_estimator.get_was_supervised(),\n",
    "        'metrics': best_estimator.get_all_performance_metrics(),\n",
    "        'size_of_dataset': size_of_dataset,\n",
    "        'use_only_CARD': use_only_card\n",
    "    }\n",
    "    # Prepend the value 'param' to the keys in params\n",
    "    for key, value in param.items():\n",
    "        local_key = 'params.' + key\n",
    "        output[local_key] = value\n",
    "\n",
    "    results.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m viz \u001b[38;5;241m=\u001b[39m Visualizer()\n\u001b[0;32m----> 2\u001b[0m df_with_two_dimens \u001b[38;5;241m=\u001b[39m viz\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mcluster_dfs\u001b[49m[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_with_two_dimens\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster_dfs' is not defined"
     ]
    }
   ],
   "source": [
    "viz = Visualizer()\n",
    "df_with_two_dimens = viz.fit_transform(cluster_dfs[0], 'embeddings')\n",
    "df_with_two_dimens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件输入输出模板\n",
    "\n",
    "import boto3\n",
    "\n",
    "# 创建 S3 客户端\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'sagemaker-us-east-1-390403859474'\n",
    "\n",
    "s3_file_key = 'processed_file.csv'  # S3 中的文件路径\n",
    "\n",
    "local_file_path = 'downloaded_file.csv'  # 本地保存的文件名\n",
    "\n",
    "# 下载文件\n",
    "s3.download_file(bucket_name, s3_file_key, local_file_path)\n",
    "\n",
    "print(f\"File {s3_file_key} has been downloaded to {local_file_path}.\")\n",
    "\n",
    "processed_file_path = f'results_${str(now)}.csv'\n",
    "results_df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# 定义上传路径\n",
    "upload_file_key = 'processed_file.csv'  # 上传到 S3 的路径\n",
    "\n",
    "# 上传文件\n",
    "s3.upload_file(processed_file_path, bucket_name, upload_file_key)\n",
    "\n",
    "print(f\"File {processed_file_path} has been uploaded to s3://{bucket_name}/{upload_file_key}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator.clusters_df.head(100).to_csv(\"test1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Get access, secret key, and bucket name from environment variables\n",
    "AWS_ACCESS_KEY = os.environ['AWS_ACCESS_KEY']\n",
    "AWS_SECRET_KEY = os.environ['AWS_SECRET_KEY']\n",
    "BUCKET_NAME = os.environ['BUCKET_NAME']\n",
    "\n",
    "# Data File\n",
    "CSV_FILE_PATH = 'test1.csv'\n",
    "\n",
    "# Connect to S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Create local temp file\n",
    "TEMP_DIR = 'temp_files'\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Read CSV file and get column name\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "batch_size = 10\n",
    "total_batches = (len(data) + batch_size - 1) // batch_size\n",
    "upload_count = 0\n",
    "\n",
    "for i in range(total_batches):\n",
    "    \n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(data))\n",
    "    df = data.iloc[start_idx:end_idx]\n",
    "    column_names = df.columns\n",
    "    \n",
    "    # Create files contains single claim\n",
    "    for index, row in df.iterrows():\n",
    "        text = str(df.loc[index, 'text'])\n",
    "        filtered_text = ''.join(re.findall(r'[A-Za-z0-9]', text))\n",
    "        sub_name = filtered_text[:30]\n",
    "        file_name = f\"claim_{sub_name}.csv\"\n",
    "        file_path = os.path.join(TEMP_DIR, file_name)\n",
    "        \n",
    "        single_row_df = pd.DataFrame([row], columns=column_names)\n",
    "        single_row_df.to_csv(file_path, index=False)\n",
    "        \n",
    "        s3_key = f\"groundtruth/{file_name}\"\n",
    "        \n",
    "        # Check if the file exists in S3\n",
    "        try:\n",
    "            s3_client.head_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "            # File exists, download it\n",
    "            s3_object = s3_client.get_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "            s3_data = pd.read_csv(io.BytesIO(s3_object['Body'].read()))\n",
    "            \n",
    "            # Compare the data\n",
    "            if single_row_df.equals(s3_data):\n",
    "                print(f\"{file_name} is up-to-date. Skipping upload.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"{file_name} is outdated. Uploading new version.\")\n",
    "        except s3_client.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                # File does not exist\n",
    "                print(f\"{file_name} does not exist in S3. Uploading new file.\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_client.upload_file(\n",
    "            Filename=file_path,\n",
    "            Bucket=BUCKET_NAME,\n",
    "            Key=s3_key  # S3 path\n",
    "        )\n",
    "        print(f\"Uploaded {file_name} to S3.\")\n",
    "        upload_count += 1\n",
    "\n",
    "        # Rest if upload count equals batch_size\n",
    "        if upload_count == batch_size:\n",
    "            print(\"Rest for 180s...\")\n",
    "            time.sleep(180)\n",
    "            upload_count = 0\n",
    "\n",
    "# Delete temp files\n",
    "for file in os.listdir(TEMP_DIR):\n",
    "    os.remove(os.path.join(TEMP_DIR, file))\n",
    "os.rmdir(TEMP_DIR)\n",
    "\n",
    "print(\"All files uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# Get S3 and object key from environment variables\n",
    "bucket = os.environ['BUCKET_S3']\n",
    "bucket_key = os.environ['BUCKET_S3_KEY']\n",
    "\n",
    "\n",
    "# 使用 boto3 客户端\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# 下载文件到本地\n",
    "s3.download_file(bucket, key, 'data.json.gz')\n",
    "\n",
    "# 解压缩并逐行读取数据\n",
    "data = []\n",
    "with gzip.open('data.json.gz', 'rt') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# 如果数据为 DataFrame 格式的，可以转为 pandas DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "df.to_csv(\"GoogleFactCheckData.csv\")\n",
    "\n",
    "# 查看数据\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
