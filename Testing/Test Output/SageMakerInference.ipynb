{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n",
      "Downloading gensim-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m131.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: smart-open, scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.14.1\n",
      "    Uninstalling scipy-1.14.1:\n",
      "      Successfully uninstalled scipy-1.14.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1 smart-open-7.1.0\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "Collecting openai\n",
      "  Downloading openai-1.60.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (0.28.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (2.10.3)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
      "Downloading openai-1.60.0-py3-none-any.whl (456 kB)\n",
      "Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.8.2 openai-1.60.0\n",
      "Collecting pinecone\n",
      "  Downloading pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (2024.8.30)\n",
      "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone)\n",
      "  Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (2.9.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pinecone) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
      "Downloading pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
      "Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
      "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone\n",
      "Successfully installed pinecone-5.4.2 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (1.5.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (0.60.0)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: pynndescent, umap-learn\n",
      "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n",
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from scikit-learn>=0.20->hdbscan) (3.5.0)\n",
      "Downloading hdbscan-0.8.40-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hdbscan\n",
      "Successfully installed hdbscan-0.8.40\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install pinecone\n",
    "!pip install umap-learn\n",
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 15:37:14.339763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-22 15:37:19.753632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-22 15:37:19.817813: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-22 15:37:27.500445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-22 15:37:39.402901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the folder containing ClusterAndPredict\n",
    "module_path = os.path.abspath(os.path.join('/home/ec2-user/SageMaker', 'FactChecker-V3'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "from ClusterAndPredict.ClusterAndPredict import ClusterAndPredict\n",
    "from Testing.DataLoader import DataLoader\n",
    "from Testing.ParameterCreator import ParameterCreator\n",
    "from Clustering.Helpers.Visualizer import Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "# chroma_client = chromadb.PersistentClient(path=\"./../../Clustering/Clustering/Chroma\")\n",
    "# # Count number of collections\n",
    "# print(chroma_client.count_collections())\n",
    "# \n",
    "# # Get all collection names\n",
    "# collection_names = chroma_client.list_collections()\n",
    "# \n",
    "# # Loop through each collection and drop it\n",
    "# for collection_name in collection_names:\n",
    "#     if collection_name.name != 'climate_claims_embeddings_unchanged':\n",
    "#         chroma_client.delete_collection(collection_name.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data sources are being used\n",
      "1\n",
      "Number of experiments to run:  1\n",
      "Fitting\n",
      "Getting embeddings...\n",
      "Getting embeddings for batch  0  out of  36\n",
      "Getting embeddings for batch  0  out of  403\n",
      "Getting embeddings for batch  50  out of  403\n",
      "Getting embeddings for batch  100  out of  403\n",
      "Getting embeddings for batch  150  out of  403\n",
      "Getting embeddings for batch  200  out of  403\n",
      "Getting embeddings for batch  250  out of  403\n",
      "Getting embeddings for batch  300  out of  403\n",
      "Getting embeddings for batch  350  out of  403\n",
      "Getting embeddings for batch  400  out of  403\n",
      "temp_df length: 439\n",
      "claims_embeddings shape:  (36, 3072)\n",
      "claims_embeddings predict shape:  (403, 3072)\n",
      "Running parametric supervised umap...\n",
      "seed: 23\n",
      "Transforming time: 0.6328814029693604 seconds\n",
      "Running hdbscan...\n",
      "breaking further...\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "cluster  predict  veracity\n",
      "-1       False     3           86\n",
      "                   1           61\n",
      "         True     -1           27\n",
      " 2       False     3            5\n",
      " 3       False     3            9\n",
      "         True     -1            3\n",
      " 4       False     3            7\n",
      " 5       False     3            6\n",
      " 6       False     3            5\n",
      " 8       False     1           20\n",
      "         True     -1            1\n",
      " 11      False     1           63\n",
      "                   3            5\n",
      "         True     -1            2\n",
      " 12      False     1           27\n",
      "                   3            2\n",
      "         True     -1            2\n",
      " 13      False     3          107\n",
      "         True     -1            1\n",
      "Name: veracity, dtype: int64\n",
      "breaking further...\n",
      "cluster  predict  veracity\n",
      "-1       False     3           86\n",
      "                   1           61\n",
      "         True     -1           27\n",
      " 2       False     3            5\n",
      " 3       False     3            9\n",
      "         True     -1            3\n",
      " 4       False     3            7\n",
      " 5       False     3            6\n",
      " 6       False     3            5\n",
      " 8       False     1           20\n",
      "         True     -1            1\n",
      " 11      False     1           63\n",
      "                   3            5\n",
      "         True     -1            2\n",
      " 12      False     1           27\n",
      "                   3            2\n",
      "         True     -1            2\n",
      " 13      False     3          107\n",
      "         True     -1            1\n",
      "Name: veracity, dtype: int64\n",
      "Number of clusters: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:538: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df.loc[(cluster_df['predicted_veracity'] == 4) & (cluster_df['veracity'] == 1), 'predicted_veracity'] = 3\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:571: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 3)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:572: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:578: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 1)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:579: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:585: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall(cluster_df)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:586: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:538: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df.loc[(cluster_df['predicted_veracity'] == 4) & (cluster_df['veracity'] == 1), 'predicted_veracity'] = 3\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:571: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 3)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:572: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:578: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 1)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:579: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:585: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall(cluster_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happened\n",
      "ERROR DEBUG\n",
      "1    36\n",
      "Name: veracity, dtype: int64\n",
      "3    31\n",
      "1     5\n",
      "Name: predicted_veracity, dtype: int64\n",
      "happened\n",
      "ERROR DEBUG\n",
      "1    9\n",
      "Name: veracity, dtype: int64\n",
      "1    5\n",
      "3    4\n",
      "Name: predicted_veracity, dtype: int64\n",
      "happened\n",
      "ERROR DEBUG\n",
      "1    36\n",
      "Name: veracity, dtype: int64\n",
      "3    31\n",
      "1     5\n",
      "Name: predicted_veracity, dtype: int64\n",
      "happened\n",
      "ERROR DEBUG\n",
      "1    9\n",
      "Name: veracity, dtype: int64\n",
      "1    5\n",
      "3    4\n",
      "Name: predicted_veracity, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:586: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:538: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df.loc[(cluster_df['predicted_veracity'] == 4) & (cluster_df['veracity'] == 1), 'predicted_veracity'] = 3\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:571: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 3)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:572: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:578: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 1)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:579: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:585: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall(cluster_df)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:586: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happened\n",
      "ERROR DEBUG\n",
      "1    36\n",
      "Name: veracity, dtype: int64\n",
      "3    31\n",
      "1     5\n",
      "Name: predicted_veracity, dtype: int64\n",
      "happened\n",
      "ERROR DEBUG\n",
      "1    9\n",
      "Name: veracity, dtype: int64\n",
      "1    5\n",
      "3    4\n",
      "Name: predicted_veracity, dtype: int64\n",
      "happened\n",
      "ERROR DEBUG\n",
      "1    36\n",
      "Name: veracity, dtype: int64\n",
      "3    31\n",
      "1     5\n",
      "Name: predicted_veracity, dtype: int64\n",
      "happened\n",
      "ERROR DEBUG\n",
      "1    9\n",
      "Name: veracity, dtype: int64\n",
      "1    5\n",
      "3    4\n",
      "Name: predicted_veracity, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:482: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Filter df to where veracity is value\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:483: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df = cluster_df[cluster_df['veracity'] == value]\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:538: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cluster_df.loc[(cluster_df['predicted_veracity'] == 4) & (cluster_df['veracity'] == 1), 'predicted_veracity'] = 3\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:571: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 3)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:572: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:578: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall_for_a_value(cluster_df, 1)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:579: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:585: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self.calculate_precision_recall(cluster_df)\n",
      "/home/ec2-user/SageMaker/FactChecker-V3/ClusterAndPredict/ClusterAndPredict.py:586: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "params = ParameterCreator().get_parameters()\n",
    "results = []\n",
    "cluster_dfs = []\n",
    "print(\"Number of experiments to run: \", len(params))\n",
    "for param in params:\n",
    "    percentage = 0.75\n",
    "    data_loader = DataLoader(percentage, True, param['random_seed'])\n",
    "    use_only_card = param['use_only_CARD']\n",
    "    size_of_dataset = param['size_of_dataset']\n",
    "    del param['size_of_dataset']\n",
    "    del param['use_only_CARD']\n",
    "    \n",
    "    train_df, test_df = data_loader.create_reddit_dataset()\n",
    "\n",
    "    # train_df, test_df = data_loader.create_train_test_df(False, False, False, True, size_of_dataset)\n",
    "    # if use_only_card:\n",
    "    #     print(\"using card\")\n",
    "    #     train_df, test_df = data_loader.create_train_test_df(True, True, True, True, size_of_dataset)\n",
    "\n",
    "    clf = ClusterAndPredict(**param, train_df=train_df)\n",
    "    clf.fit(test_df['Text'].tolist(), test_df['Numerical Rating'].tolist())\n",
    "    # Print best parameters\n",
    "    best_estimator = clf\n",
    "    score = best_estimator.score([], [])\n",
    "    object_output = best_estimator.get_all_performance_metrics()\n",
    "    cluster_df = object_output['cluster_df']\n",
    "    # cluster_df = clf.generate_explanations_and_similar_for_each_claim(cluster_df, 'predicted_veracity', 'cluster', 'text')\n",
    "    test_df['Source'] = 'Prediction from model'\n",
    "    combined_df = pd.concat([train_df[['Text', 'Source']], test_df[['Text', 'Source']]])\n",
    "    cluster_df = pd.merge(cluster_df, combined_df, left_on='text', right_on='Text', how='left')\n",
    "    cluster_df.drop(columns=['Text'], inplace=True)\n",
    "    cluster_dfs.append(cluster_df)\n",
    "    output = {\n",
    "        'percentage': percentage,\n",
    "        'score': score,\n",
    "        'accuracy': best_estimator.get_accuracy(),\n",
    "        'was_supervised_umap_used': best_estimator.get_was_supervised(),\n",
    "        'metrics': best_estimator.get_all_performance_metrics(),\n",
    "        'size_of_dataset': size_of_dataset,\n",
    "        'use_only_CARD': use_only_card\n",
    "    }\n",
    "    # Prepend the value 'param' to the keys in params\n",
    "    for key, value in param.items():\n",
    "        local_key = 'params.' + key\n",
    "        output[local_key] = value\n",
    "\n",
    "    results.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>veracity</th>\n",
       "      <th>predict</th>\n",
       "      <th>predicted_veracity</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>cluster</th>\n",
       "      <th>num_correct_in_cluster</th>\n",
       "      <th>total_in_cluster</th>\n",
       "      <th>cluster_accuracy</th>\n",
       "      <th>Source</th>\n",
       "      <th>reduced_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Its emissions is more than the emissions of 14...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.16720734536647797, 0.3540584146976471, -0....</td>\n",
       "      <td>-1</td>\n",
       "      <td>147</td>\n",
       "      <td>174</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>Prediction from model</td>\n",
       "      <td>[1.2286924, 5.118195]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China banned CFCs in 2010, as per what they an...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.20001590251922607, 0.4432351291179657, -1....</td>\n",
       "      <td>-1</td>\n",
       "      <td>147</td>\n",
       "      <td>174</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>Prediction from model</td>\n",
       "      <td>[0.73341674, 4.6134048]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Climate Models](https://www.heritage.org/envir...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.4435594975948334, 0.30042654275894165, -0.2...</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Prediction from model</td>\n",
       "      <td>[9.652608, 8.03992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And then there's this monstrous inconsistency ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.10906676948070526, -0.027161184698343277, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>147</td>\n",
       "      <td>174</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>Prediction from model</td>\n",
       "      <td>[4.241845, 7.897852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;[Bungling World Bank bureaucrats ](https://ny...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.1842391937971115, 0.5911382436752319, -0.85...</td>\n",
       "      <td>11</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Prediction from model</td>\n",
       "      <td>[-0.46748203, 5.271638]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Increases in average and extreme temperatures ...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.27104878425598145, 0.5601314306259155, -0.7...</td>\n",
       "      <td>13</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>EPA</td>\n",
       "      <td>[12.006522, 3.2026904]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>About 80% of the ground in Alaska has permafro...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.1546349674463272, 0.0839909166097641, -0.5...</td>\n",
       "      <td>-1</td>\n",
       "      <td>147</td>\n",
       "      <td>174</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>EPA</td>\n",
       "      <td>[3.0316122, 6.38583]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>When a tree came in contact with electrical di...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.5470941662788391, 0.7620251774787903, -0.75...</td>\n",
       "      <td>-1</td>\n",
       "      <td>147</td>\n",
       "      <td>174</td>\n",
       "      <td>0.844828</td>\n",
       "      <td>EPA</td>\n",
       "      <td>[12.635266, 5.6658316]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>They also breathe at a faster rate, increasing...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.21261060237884521, 0.5829666256904602, -0.8...</td>\n",
       "      <td>13</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>EPA</td>\n",
       "      <td>[7.4739237, 1.9064482]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>They also recover in a few days to a week. Tra...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.32316285371780396, 0.5745985507965088, -0.7...</td>\n",
       "      <td>13</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>EPA</td>\n",
       "      <td>[12.2726, 3.5243485]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  veracity  predict  \\\n",
       "0    Its emissions is more than the emissions of 14...         1     True   \n",
       "1    China banned CFCs in 2010, as per what they an...         1     True   \n",
       "2    Climate Models](https://www.heritage.org/envir...         1     True   \n",
       "3    And then there's this monstrous inconsistency ...         1     True   \n",
       "4    >[Bungling World Bank bureaucrats ](https://ny...         1     True   \n",
       "..                                                 ...       ...      ...   \n",
       "434  Increases in average and extreme temperatures ...         3    False   \n",
       "435  About 80% of the ground in Alaska has permafro...         3    False   \n",
       "436  When a tree came in contact with electrical di...         3    False   \n",
       "437  They also breathe at a faster rate, increasing...         3    False   \n",
       "438  They also recover in a few days to a week. Tra...         3    False   \n",
       "\n",
       "     predicted_veracity                                         embeddings  \\\n",
       "0                     4  [-0.16720734536647797, 0.3540584146976471, -0....   \n",
       "1                     4  [-0.20001590251922607, 0.4432351291179657, -1....   \n",
       "2                     1  [0.4435594975948334, 0.30042654275894165, -0.2...   \n",
       "3                     4  [-0.10906676948070526, -0.027161184698343277, ...   \n",
       "4                     1  [0.1842391937971115, 0.5911382436752319, -0.85...   \n",
       "..                  ...                                                ...   \n",
       "434                   3  [0.27104878425598145, 0.5601314306259155, -0.7...   \n",
       "435                   3  [-0.1546349674463272, 0.0839909166097641, -0.5...   \n",
       "436                   3  [0.5470941662788391, 0.7620251774787903, -0.75...   \n",
       "437                   3  [0.21261060237884521, 0.5829666256904602, -0.8...   \n",
       "438                   3  [0.32316285371780396, 0.5745985507965088, -0.7...   \n",
       "\n",
       "     cluster  num_correct_in_cluster  total_in_cluster  cluster_accuracy  \\\n",
       "0         -1                     147               174          0.844828   \n",
       "1         -1                     147               174          0.844828   \n",
       "2          8                      21                21          1.000000   \n",
       "3         -1                     147               174          0.844828   \n",
       "4         11                      70                70          1.000000   \n",
       "..       ...                     ...               ...               ...   \n",
       "434       13                     107               108          0.990741   \n",
       "435       -1                     147               174          0.844828   \n",
       "436       -1                     147               174          0.844828   \n",
       "437       13                     107               108          0.990741   \n",
       "438       13                     107               108          0.990741   \n",
       "\n",
       "                    Source       reduced_embeddings  \n",
       "0    Prediction from model    [1.2286924, 5.118195]  \n",
       "1    Prediction from model  [0.73341674, 4.6134048]  \n",
       "2    Prediction from model      [9.652608, 8.03992]  \n",
       "3    Prediction from model     [4.241845, 7.897852]  \n",
       "4    Prediction from model  [-0.46748203, 5.271638]  \n",
       "..                     ...                      ...  \n",
       "434                    EPA   [12.006522, 3.2026904]  \n",
       "435                    EPA     [3.0316122, 6.38583]  \n",
       "436                    EPA   [12.635266, 5.6658316]  \n",
       "437                    EPA   [7.4739237, 1.9064482]  \n",
       "438                    EPA     [12.2726, 3.5243485]  \n",
       "\n",
       "[439 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viz = Visualizer()\n",
    "df_with_two_dimens = viz.fit_transform(cluster_dfs[0], 'embeddings')\n",
    "# df_with_two_dimens = df_with_two_dimens.drop('embeddings', axis=1)\n",
    "cluster_dfs[0] = df_with_two_dimens\n",
    "df_with_two_dimens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating explanations: 100%|██████████| 439/439 [00:15<00:00, 27.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File results_2025-01-22_16-31-37.csv has been uploaded to s3://sagemaker-us-east-1-390403859474/processed_file.csv.\n",
      "File processed_files/processed_file_0.json has been uploaded to s3://sagemaker-us-east-1-390403859474/processed_files/processed_file_0.json.\n",
      "All chunks and the entire CSV file have been processed and uploaded.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'sagemaker-us-east-1-390403859474'\n",
    "s3_file_key = 'processed_file.csv'  # File path in S3\n",
    "local_file_path = 'downloaded_file.csv'  # Local file name to save\n",
    "\n",
    "# Download file (commented out as per original code)\n",
    "# s3.download_file(bucket_name, s3_file_key, local_file_path)\n",
    "# print(f\"File {s3_file_key} has been downloaded to {local_file_path}.\")\n",
    "cluster_dfs[0].to_csv(local_file_path)\n",
    "\n",
    "# Read CSV file\n",
    "results_df = pd.read_csv(local_file_path)\n",
    "results_df = clf.clean_columns_for_s3(results_df)\n",
    "results_df = clf.generate_explanations_and_similar_for_each_claim(results_df, 'predicted_veracity', 'cluster', 'text', True)\n",
    "\n",
    "# Process file path for the entire CSV\n",
    "processed_file_path = f'results_{pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.csv'\n",
    "results_df.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Upload the entire CSV file\n",
    "upload_file_key = 'processed_file.csv'  # Path to upload to S3\n",
    "s3.upload_file(processed_file_path, bucket_name, upload_file_key)\n",
    "print(f\"File {processed_file_path} has been uploaded to s3://{bucket_name}/{upload_file_key}.\")\n",
    "\n",
    "# Create a local directory for processed files\n",
    "os.makedirs('processed_files', exist_ok=True)\n",
    "\n",
    "# Split DataFrame into 1000-row chunks\n",
    "chunk_size = 1000\n",
    "chunks = [results_df.iloc[i:i + chunk_size] for i in range(0, len(results_df), chunk_size)]\n",
    "\n",
    "# Process and upload each chunk\n",
    "for index, chunk in enumerate(chunks):\n",
    "    # Generate local file path for the chunk\n",
    "    local_chunk_path = f'processed_files/processed_file_{index}.json'\n",
    "    \n",
    "    # Save chunk to local file\n",
    "    chunk.to_json(local_chunk_path)\n",
    "    \n",
    "    # Generate S3 key for the chunk\n",
    "    s3_chunk_key = f'processed_files/processed_file_{index}.json'\n",
    "    \n",
    "    # Upload chunk to S3\n",
    "    s3.upload_file(local_chunk_path, bucket_name, s3_chunk_key)\n",
    "    print(f\"File {local_chunk_path} has been uploaded to s3://{bucket_name}/{s3_chunk_key}.\")\n",
    "\n",
    "print(\"All chunks and the entire CSV file have been processed and uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opensearch-py in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,!=2.2.1,<3,>=1.26.19 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from opensearch-py) (1.26.19)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from opensearch-py) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from opensearch-py) (2.9.0)\n",
      "Requirement already satisfied: certifi>=2024.07.04 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from opensearch-py) (2024.8.30)\n",
      "Requirement already satisfied: Events in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from opensearch-py) (0.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.0->opensearch-py) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil->opensearch-py) (1.17.0)\n",
      "Collecting requests-aws4auth\n",
      "  Downloading requests_aws4auth-1.3.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests-aws4auth) (2.32.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->requests-aws4auth) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->requests-aws4auth) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->requests-aws4auth) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->requests-aws4auth) (2024.8.30)\n",
      "Downloading requests_aws4auth-1.3.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: requests-aws4auth\n",
      "Successfully installed requests-aws4auth-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install opensearch-py\n",
    "!pip install requests-aws4auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OpenSearchService' object has no attribute 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 52\u001b[0m\n\u001b[1;32m     19\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_vector_index\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     20\u001b[0m index_body \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmappings\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m }\n\u001b[0;32m---> 52\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[38;5;241m.\u001b[39mcreate(index\u001b[38;5;241m=\u001b[39mindex_name, body\u001b[38;5;241m=\u001b[39mindex_body)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopensearchpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bulk\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_actions\u001b[39m():\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/botocore/client.py:922\u001b[0m, in \u001b[0;36mBaseClient.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m event_response\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    924\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OpenSearchService' object has no attribute 'indices'"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "\n",
    "host = 'qwrgattdrsmspf43ydw6.us-east-1.aoss.amazonaws.com'\n",
    "region = 'us-east-1'\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "index_name = 'my_vector_index'\n",
    "index_body = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'id': {'type': 'keyword'},\n",
    "            'text': {'type': 'text'},\n",
    "            'veracity': {'type': 'text'},\n",
    "            'predict': {'type': 'boolean'},\n",
    "            'predicted_veracity': {'type': 'text'},\n",
    "            'cluster': {'type': 'text'},\n",
    "            'source': {'type': 'text'},\n",
    "            'embeddings': {\n",
    "                'type': 'knn_vector',\n",
    "                'dimension': 100,\n",
    "                'method': {\n",
    "                    'name': 'hnsw',\n",
    "                    'space_type': 'l2',\n",
    "                    'engine': 'nmslib'\n",
    "                }\n",
    "            },\n",
    "            'reduced_embeddings': {\n",
    "                'type': 'knn_vector',\n",
    "                'dimension': 2,\n",
    "                'method': {\n",
    "                    'name': 'hnsw',\n",
    "                    'space_type': 'l2',\n",
    "                    'engine': 'nmslib'\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "client.indices.create(index=index_name, body=index_body)\n",
    "\n",
    "from opensearchpy.helpers import bulk\n",
    "\n",
    "def generate_actions():\n",
    "    for _, row in df.iterrows():\n",
    "        text = str(row['text'])\n",
    "        filtered_text = ''.join(re.findall(r'[A-Za-z0-9]', text))\n",
    "        id_to_use = filtered_text[:30]\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"_source\": {\n",
    "                \"id\": id_to_use,\n",
    "                \"text\": row['text'],\n",
    "                \"veracity\": row['veracity'],\n",
    "                \"predict\": row['predict'],\n",
    "                \"predicted_veracity\": row['predicted_veracity'],\n",
    "                \"cluster\": row['cluster'],\n",
    "                \"source\": row['Source'],\n",
    "                \"reduced_embeddings\": row['reduced_embeddings'],\n",
    "                \"embedding\": row['embedding'].tolist()  # Convert numpy array to list\n",
    "            }\n",
    "        }\n",
    "\n",
    "success, _ = bulk(client, generate_actions())\n",
    "print(f\"Indexed {success} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_estimator.clusters_df.head(100).to_csv(\"test1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Get access, secret key, and bucket name from environment variables\n",
    "AWS_ACCESS_KEY = os.environ['AWS_ACCESS_KEY']\n",
    "AWS_SECRET_KEY = os.environ['AWS_SECRET_KEY']\n",
    "BUCKET_NAME = os.environ['BUCKET_NAME']\n",
    "\n",
    "# Data File\n",
    "CSV_FILE_PATH = processed_file_path\n",
    "\n",
    "# Connect to S3\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_KEY\n",
    ")\n",
    "\n",
    "# Create local temp file\n",
    "TEMP_DIR = 'temp_files'\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "\n",
    "# Read CSV file and get column name\n",
    "data = pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "batch_size = 1250\n",
    "total_batches = (len(data) + batch_size - 1) // batch_size\n",
    "upload_count = 0\n",
    "\n",
    "for i in range(total_batches):\n",
    "    \n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(data))\n",
    "    df = data.iloc[start_idx:end_idx]\n",
    "    column_names = df.columns\n",
    "    \n",
    "    # Create files contains single claim\n",
    "    for index, row in df.iterrows():\n",
    "        text = str(df.loc[index, 'text'])\n",
    "        filtered_text = ''.join(re.findall(r'[A-Za-z0-9]', text))\n",
    "        sub_name = filtered_text[:30]\n",
    "        file_name = f\"claim_{sub_name}.csv\"\n",
    "        file_path = os.path.join(TEMP_DIR, file_name)\n",
    "        \n",
    "        single_row_df = pd.DataFrame([row], columns=column_names)\n",
    "        single_row_df.to_csv(file_path, index=False)\n",
    "        \n",
    "        s3_key = f\"groundtruth/{file_name}\"\n",
    "        \n",
    "        # Check if the file exists in S3\n",
    "        try:\n",
    "            s3_client.head_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "            # File exists, download it\n",
    "            s3_object = s3_client.get_object(Bucket=BUCKET_NAME, Key=s3_key)\n",
    "            s3_data = pd.read_csv(io.BytesIO(s3_object['Body'].read()))\n",
    "            \n",
    "            old_prediction = single_row_df['predicted_veracity'].iloc[0]\n",
    "            new_prediction = s3_data['predicted_veracity'].iloc[0]\n",
    "            \n",
    "            # Compare the data\n",
    "            if old_prediction == new_prediction and False:\n",
    "                print(f\"{file_name} is up-to-date. Skipping upload.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"{file_name} is outdated. Uploading new version.\")\n",
    "        except s3_client.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] == '404':\n",
    "                # File does not exist\n",
    "                print(f\"{file_name} does not exist in S3. Uploading new file.\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Upload to S3\n",
    "        s3_client.upload_file(\n",
    "            Filename=file_path,\n",
    "            Bucket=BUCKET_NAME,\n",
    "            Key=s3_key  # S3 path\n",
    "        )\n",
    "        print(f\"Uploaded {file_name} to S3.\")\n",
    "        upload_count += 1\n",
    "\n",
    "        # Rest if upload count equals batch_size\n",
    "        if upload_count == batch_size:\n",
    "            print(\"Rest for 180s...\")\n",
    "            time.sleep(120)\n",
    "            upload_count = 0\n",
    "\n",
    "# Delete temp files\n",
    "for file in os.listdir(TEMP_DIR):\n",
    "    os.remove(os.path.join(TEMP_DIR, file))\n",
    "os.rmdir(TEMP_DIR)\n",
    "\n",
    "print(\"All files uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "# Get S3 and object key from environment variables\n",
    "bucket = os.environ['BUCKET_S3']\n",
    "bucket_key = os.environ['BUCKET_S3_KEY']\n",
    "\n",
    "\n",
    "# 使用 boto3 客户端\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# 下载文件到本地\n",
    "s3.download_file(bucket, key, 'data.json.gz')\n",
    "\n",
    "# 解压缩并逐行读取数据\n",
    "data = []\n",
    "with gzip.open('data.json.gz', 'rt') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# 如果数据为 DataFrame 格式的，可以转为 pandas DataFrame\n",
    "df = pd.json_normalize(data)\n",
    "\n",
    "df.to_csv(\"GoogleFactCheckData.csv\")\n",
    "\n",
    "# 查看数据\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunk 2/17: s3://testtestgfc/ground_t/processed_chunk_2.csv\n",
      "Uploaded chunk 3/17: s3://testtestgfc/ground_t/processed_chunk_3.csv\n",
      "Uploaded chunk 4/17: s3://testtestgfc/ground_t/processed_chunk_4.csv\n",
      "Uploaded chunk 5/17: s3://testtestgfc/ground_t/processed_chunk_5.csv\n",
      "Uploaded chunk 6/17: s3://testtestgfc/ground_t/processed_chunk_6.csv\n",
      "Uploaded chunk 7/17: s3://testtestgfc/ground_t/processed_chunk_7.csv\n",
      "Uploaded chunk 8/17: s3://testtestgfc/ground_t/processed_chunk_8.csv\n",
      "Uploaded chunk 9/17: s3://testtestgfc/ground_t/processed_chunk_9.csv\n",
      "Uploaded chunk 10/17: s3://testtestgfc/ground_t/processed_chunk_10.csv\n",
      "Uploaded chunk 11/17: s3://testtestgfc/ground_t/processed_chunk_11.csv\n",
      "Uploaded chunk 12/17: s3://testtestgfc/ground_t/processed_chunk_12.csv\n",
      "Uploaded chunk 16/17: s3://testtestgfc/ground_t/processed_chunk_16.csv\n",
      "Uploaded chunk 17/17: s3://testtestgfc/ground_t/processed_chunk_17.csv\n",
      "All data has been processed and uploaded in chunks.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import math\n",
    "import time\n",
    "\n",
    "def process_claims(claims):\n",
    "    sentences = claims.split(\"\\n\")\n",
    "    if len(sentences) > 5:\n",
    "        return \"\\n\".join(sentences[:5]) + \"...\"\n",
    "    return claims\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define bucket names and folders\n",
    "source_bucket = 'sagemaker-us-east-1-390403859474'\n",
    "source_folder = 'processed_files/'\n",
    "destination_bucket = 'testtestgfc'\n",
    "destination_folder = 'ground_t/'\n",
    "\n",
    "# List files in the source folder\n",
    "response = s3.list_objects_v2(Bucket=source_bucket, Prefix=source_folder)\n",
    "file_keys = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'] != source_folder]\n",
    "\n",
    "# Load and process all files\n",
    "all_data = []\n",
    "for file_key in file_keys:\n",
    "    obj = s3.get_object(Bucket=source_bucket, Key=file_key)\n",
    "    df = pd.read_json(io.BytesIO(obj['Body'].read()))\n",
    "    \n",
    "    if 'similar_claims' in df.columns:\n",
    "        df['similar_claims'] = df['similar_claims'].apply(process_claims)\n",
    "    \n",
    "    if 'id' in df.columns:\n",
    "        df['id'] = df['id'].astype(str).str[:100]  # Substring 'id' column to first 100 characters\n",
    "    \n",
    "    all_data.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Calculate number of chunks\n",
    "chunk_size = 500\n",
    "num_chunks = math.ceil(len(combined_df) / chunk_size)\n",
    "\n",
    "# Process and upload chunks\n",
    "for i in range(num_chunks):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(combined_df))\n",
    "    chunk = combined_df.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Save chunk to CSV buffer\n",
    "    buffer = io.StringIO()\n",
    "    chunk.to_csv(buffer, index=False)\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Generate chunk file name\n",
    "    chunk_file_name = f\"processed_chunk_{i+1}.csv\"\n",
    "    destination_key = destination_folder + chunk_file_name\n",
    "    \n",
    "    # Upload chunk to destination bucket\n",
    "    s3.put_object(Bucket=destination_bucket, Key=destination_key, Body=buffer.getvalue())\n",
    "    print(f\"Uploaded chunk {i+1}/{num_chunks}: s3://{destination_bucket}/{destination_key}\")\n",
    "    time.sleep(275)\n",
    "\n",
    "print(\"All data has been processed and uploaded in chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
